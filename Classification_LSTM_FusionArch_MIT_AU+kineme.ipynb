{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Copy of LSTM_FusionArch_Ov.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "306b002e"
      },
      "source": [
        "#import required modules\n",
        "#basic\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "from math import sqrt\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#sklearn for required metrics\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#tensorflow and other imports\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#importing keras layers and models as required\n",
        "from keras.layers import merge\n",
        "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, concatenate"
      ],
      "id": "306b002e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "268f7c86"
      },
      "source": [
        "#function to convert continous labels into binary labels\n",
        "def bin_labels(data_rec):             \n",
        "    count_0 = 0\n",
        "    count_1 = 0\n",
        "    median_value = np.median(data_rec)\n",
        "    for it in range(0,len(data_rec),1):\n",
        "        if data_rec[it]<=median_value :\n",
        "            count_0 += 1\n",
        "            data_rec[it]=0\n",
        "        else:\n",
        "            count_1 += 1\n",
        "            data_rec[it]=1\n",
        "    print(count_0, count_1)\n",
        "    return data_rec  \n",
        "\n",
        "# onehot encoding of kineme sequence\n",
        "def onehot_encoding(kineme_seq, nKineme):\n",
        "    onehot_encoded = list()\n",
        "    for each_kineme in kineme_seq:\n",
        "        vector = [0 for _ in range(nKineme)]\n",
        "        vector[each_kineme-1] = 1\n",
        "        onehot_encoded.append(vector)\n",
        "    return onehot_encoded\n",
        "\n",
        "\n",
        "def ks_encoding(kineme_seq, nKineme):\n",
        "    # ks is a numpy ndarray\n",
        "    m, n = kineme_seq.shape #m=92, n=29\n",
        "    #print(m, n)\n",
        "    kineme_seq = kineme_seq.tolist() #converted to list\n",
        "    encoded_features = np.asarray(\n",
        "        [np.asarray(onehot_encoding(kineme_seq[i], nKineme)) for i in range(m)]\n",
        "    )\n",
        "    return encoded_features"
      ],
      "id": "268f7c86",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9d98ff9"
      },
      "source": [
        "#pass on the label data path, train data path, label, sequence length and weight used in fusion\n",
        "def training_lstm(y_data_path, X_data_path, Label_class, sl, w):\n",
        "    # parameters\n",
        "    seqLen = sl\n",
        "    nKineme, nClass = 16, 1\n",
        "    nAction = 17\n",
        "    EPOCHS = 30\n",
        "    BATCH_SIZE = 32\n",
        "    nNeuron = 12\n",
        "    \n",
        "    #load y_data, convert to float and then convert the values to categorical labels as 0 or 1\n",
        "    y_data = np.load(y_data_path)\n",
        "    y_data = y_data.astype(np.float)\n",
        "    y_data = bin_labels(y_data)\n",
        "    X_data = np.load(X_data_path)\n",
        "\n",
        "\n",
        "    #model defined for kineme\n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "    Model_kineme = Sequential()\n",
        "    Model_kineme.add(LSTM(nNeuron,activation=\"tanh\",dropout=0.1,input_shape=(seqLen, nKineme)))\n",
        "    Model_kineme.add(Dense(units = nClass,activation=\"sigmoid\"))\n",
        "    opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "    Model_kineme.compile(optimizer = opt, loss = 'binary_crossentropy',metrics=['accuracy'])\n",
        "    Model_kineme.summary()\n",
        "    \n",
        "    \n",
        "    #AU model architecture\n",
        "    Model_AU = Sequential()\n",
        "    Model_AU.add(LSTM(nNeuron,activation=\"tanh\",dropout=0.1,input_shape=(seqLen, nAction)))\n",
        "    Model_AU.add(Dense(units = nClass,activation=\"sigmoid\"))\n",
        "    opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "    Model_AU.compile(optimizer = opt, loss = 'binary_crossentropy',metrics=['accuracy'])\n",
        "    Model_AU.summary()\n",
        "\n",
        "    #define the loss and accuracy lists\n",
        "    test_loss=[]\n",
        "    test_acc=[]\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "    fi_weighted=[]\n",
        "    fi_macro=[]\n",
        "    val_loss = []\n",
        "    val_acc = []\n",
        "\n",
        "    random_state = 42\n",
        "    rkf = RepeatedKFold(n_splits=10, n_repeats=5, random_state=random_state)      #repeat kfold function\n",
        "    #split X_data into train and test data for 10 fold 5 times\n",
        "    for train_idx, test_idx in rkf.split(X_data):\n",
        "        train_features, test_features, train_labels, test_labels = X_data[train_idx], X_data[test_idx], y_data[train_idx], y_data[test_idx] \n",
        "        #the first seqLen values of each list are the kineme values and we encode these kineme values both for train and test data\n",
        "        train_kinemes = ks_encoding(train_features[:,0:seqLen], nKineme)\n",
        "        test_kinemes = ks_encoding(test_features[:,0:seqLen], nKineme)\n",
        "        #rest of the values in the list are AU values, reshape the values to 500*4*17(depending on the data shape and sequence length value)\n",
        "        train_action = train_features[:, seqLen:]\n",
        "        test_action = test_features[:, seqLen:]\n",
        "        train_action = train_action.reshape((train_action.shape[0], seqLen, nAction))\n",
        "        test_action = test_action.reshape((test_action.shape[0], seqLen, nAction))\n",
        "\n",
        "        #fit the kineme and AU model\n",
        "        kineme_history = Model_kineme.fit(train_kinemes, train_labels, epochs = EPOCHS, batch_size = BATCH_SIZE, validation_split=0.1,callbacks=[callback])  #Fitting the model \n",
        "        print(\"Kineme Model Training is Done\")\n",
        "        AU_history = Model_AU.fit(train_action, train_labels, epochs = EPOCHS, batch_size = BATCH_SIZE, validation_split=0.1,callbacks=[callback])\n",
        "        print(\"AU Model Training is Done\")\n",
        "        #predict values for the training kinemes and AUs and find their weighted prediction and finally predict the label as 0 or 1 according to the final predicted value\n",
        "        train_pred_kineme=Model_kineme.predict(train_kinemes)\n",
        "        train_pred_au=Model_AU.predict(train_action) \n",
        "        final_train_pred = w*train_pred_kineme + (1-w)*train_pred_au\n",
        "        y_pred_train = ((final_train_pred > 0.5)+0).ravel()\n",
        "        #same process for the test kinemes and AUs\n",
        "        test_pred_kineme = Model_kineme.predict(test_kinemes)\n",
        "        test_pred_au = Model_AU.predict(test_action)\n",
        "        final_test_pred = w*test_pred_kineme + (1-w)*test_pred_au\n",
        "        y_pred_test = ((final_test_pred > 0.5)+0).ravel()\n",
        "        #append the values to train and test accuracy\n",
        "        train_acc.append(accuracy_score(train_labels, y_pred_train))\n",
        "        test_acc.append(accuracy_score(test_labels, y_pred_test))\n",
        "        #find the weighted and macro f1 score and return all the values\n",
        "        f1_w_epoch = f1_score(test_labels, y_pred_test, average='weighted')\n",
        "        f1_m_epoch = f1_score(test_labels, y_pred_test, average='macro')\n",
        "        fi_weighted.append(f1_w_epoch)\n",
        "        fi_macro.append(f1_m_epoch)\n",
        "    return np.asarray(train_acc), np.asarray(test_acc), np.asarray(fi_weighted), np.asarray(fi_macro)\n"
      ],
      "id": "a9d98ff9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "845136e3"
      },
      "source": [
        "#define path for the data and label files\n",
        "#the filename is defined with the chunk size and label name for the MIT dataset\n",
        "y_data_path = 'Chunk_level/Label_60_Overall.npy'\n",
        "X_data_path = 'Chunk_level/Data_60_Overall.npy'\n",
        "\n",
        "#create a weight matrix and define the accuracy and f1 score lists\n",
        "weight_matrix = np.arange(0.0, 1.01, 0.05)\n",
        "train_acc_list = list()\n",
        "test_acc_list = list()\n",
        "f1_weighted_list = list()\n",
        "f1_macro_list = list()\n",
        "train_acc_std = list()\n",
        "test_acc_std = list()\n",
        "f1_weighted_std = list()\n",
        "f1_macro_std = list()\n",
        "weight_list = list()\n",
        "i = 1\n",
        "#start the loop for executing the fusion architecture using each weight\n",
        "#append all the mean and std values to the created lists\n",
        "for each_w in weight_matrix:\n",
        "    print(\"*********************\"+ str(i) + \"*********************\") \n",
        "    final_train_acc, final_test_acc, final_f1_weighted, final_f1_macro = training_lstm(y_data_path, X_data_path, \"Overall: 59\", 59, each_w)\n",
        "    weight_list.append(each_w)\n",
        "    train_acc_list.append(final_train_acc.mean())\n",
        "    train_acc_std.append(final_train_acc.std())\n",
        "    test_acc_list.append(final_test_acc.mean())\n",
        "    test_acc_std.append(final_test_acc.std())\n",
        "    f1_weighted_list.append(final_f1_weighted.mean())\n",
        "    f1_weighted_std.append(final_f1_weighted.std())\n",
        "    f1_macro_list.append(final_f1_macro.mean())\n",
        "    f1_macro_std.append(final_f1_macro.std())\n",
        "    i += 1\n",
        "\n",
        "#create a dataframe to store all the lists and dave the dataframe    \n",
        "Fusion_accuraciesO = pd.DataFrame(list(zip(weight_list, train_acc_list, train_acc_std, test_acc_list, test_acc_std,\n",
        "                           f1_weighted_list, f1_weighted_std, f1_macro_list, f1_macro_std)) , columns =['Weight value(Kineme)', \n",
        "                            'Training accuracy', 'Train std', 'Testing accuracy', 'Test std', 'F1 weighted', 'F1 weighted std', 'F1 Macro', 'F1 macro std'])\n",
        "\n",
        "\n",
        "Fusion_accuraciesO.to_csv(\"DataFrames/Overall60.csv\", index = False)\n",
        "Fusion_accuraciesO"
      ],
      "id": "845136e3",
      "execution_count": null,
      "outputs": []
    }
  ]
}